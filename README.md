Implementing a basic neural network from scratch and comparing it with Scikit-Learn's MLP Regressor.

Study 1: Basic performance comparison
In this study we compare basic stochastic gradient descent with a similar implementation of Sklearn. Sklearn outperforms due to inherent optimizations in implementation.

Study 2: Comparison of dataset size against performance
Sklearn takes more time due to persistent search for optimum solution.

Study parameters:
1. Sigmoid activation function
2. One input layer with three features
3. One hidden layer with four neurons
4. One output layer (regression task)


![Figure_1](https://github.com/GaganCodes/neural_network_scratch/assets/76858849/3df81a0f-e356-4cc4-b304-161a2af468fc)
![Figure_2](https://github.com/GaganCodes/neural_network_scratch/assets/76858849/4a5581c9-f1d7-460f-9795-76997bbb3d25)
